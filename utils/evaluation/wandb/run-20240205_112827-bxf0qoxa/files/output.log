wandb: Streaming LlamaIndex events to W&B at https://wandb.ai/arunk777/gemini-test/runs/bxf0qoxa
wandb: `WandbCallbackHandler` is currently in beta.
wandb: Please report any issues to https://github.com/wandb/wandb/issues with the tag `llamaindex`.
[nltk_data] Downloading package punkt to /Users/asanthan/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /Users/asanthan/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 36.93it/s]


100%|██████████| 8/8 [00:07<00:00,  1.02it/s]
100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
100%|██████████| 2/2 [00:03<00:00,  1.86s/it]

100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
100%|██████████| 2/2 [00:02<00:00,  1.18s/it]

100%|██████████| 2/2 [00:03<00:00,  1.87s/it]

100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
100%|██████████| 2/2 [00:02<00:00,  1.39s/it]

100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
What is the maximum number of axes that can be supported by a single Power Interface Module (PIM) in the ArmorKinetix system?
Traceback (most recent call last):
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/utils/evaluation/rag_evaluation.py", line 125, in <module>
    loop = asyncio.run(batch())
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/utils/evaluation/rag_evaluation.py", line 111, in batch
    result = evaluator.evaluate(
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/evaluation/base.py", line 62, in evaluate
    return asyncio.run(
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/nest_asyncio.py", line 30, in run
    return loop.run_until_complete(task)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/nest_asyncio.py", line 98, in run_until_complete
    return f.result()
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/evaluation/correctness.py", line 134, in aevaluate
    eval_response = await self._service_context.llm.apredict(
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/llms/llm.py", line 280, in apredict
    chat_response = await self.achat(messages)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/llms/base.py", line 57, in wrapped_async_llm_chat
    f_return_val = await f(_self, messages, **kwargs)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/llms/vertex.py", line 291, in achat
    chat_history = _parse_chat_history(messages[:-1], self._is_gemini)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/llama_index/llms/vertex_utils.py", line 174, in _parse_chat_history
    raise ValueError("Gemini model don't support system messages")
ValueError: Gemini model don't support system messages