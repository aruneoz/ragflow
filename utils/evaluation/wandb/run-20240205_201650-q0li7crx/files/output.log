wandb: Streaming LlamaIndex events to W&B at https://wandb.ai/arunk777/gemini-test/runs/q0li7crx
wandb: `WandbCallbackHandler` is currently in beta.
wandb: Please report any issues to https://github.com/wandb/wandb/issues with the tag `llamaindex`.
[nltk_data] Downloading package punkt to /Users/asanthan/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /Users/asanthan/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
Parsing nodes: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]




100%|██████████| 8/8 [00:09<00:00,  1.19s/it]
100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
100%|██████████| 2/2 [00:01<00:00,  1.78it/s]
100%|██████████| 2/2 [00:00<00:00,  2.18it/s]
100%|██████████| 2/2 [00:01<00:00,  1.19it/s]

100%|██████████| 2/2 [00:02<00:00,  1.47s/it]
100%|██████████| 2/2 [00:01<00:00,  1.25it/s]
100%|██████████| 2/2 [00:01<00:00,  1.62it/s]

100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Traceback (most recent call last):
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/utils/evaluation/rag_evaluation.py", line 178, in <module>
    loop = asyncio.run(batch())
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/Users/asanthan/.pyenv/versions/3.10.10/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/utils/evaluation/rag_evaluation.py", line 131, in batch
    result = evaluate(query_engine, metrics, eval_questions)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/ragas/llama_index/evaluation.py", line 99, in evaluate
    result = ragas_evaluate(ds, metrics)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/ragas/evaluation.py", line 88, in evaluate
    validate_evaluation_modes(dataset, metrics)
  File "/Users/asanthan/work/development/greenfield/demos/generative-ai/RAG/venv/lib/python3.10/site-packages/ragas/validation.py", line 63, in validate_evaluation_modes
    raise ValueError(
ValueError: The metric [context_recall] that that is used requires the following additional columns ['ground_truths'] to be present in the dataset.